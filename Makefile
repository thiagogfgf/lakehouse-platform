.PHONY: help setup up down restart logs clean clean-all validate demo

# Default target
.DEFAULT_GOAL := help

# Load environment variables (if exists)
-include .env
export

##@ General

help: ## Display this help message
	@awk 'BEGIN {FS = ":.*##"; printf "\nUsage:\n  make \033[36m<target>\033[0m\n"} /^[a-zA-Z_-]+:.*?##/ { printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2 } /^##@/ { printf "\n\033[1m%s\033[0m\n", substr($$0, 5) } ' $(MAKEFILE_LIST)

##@ Setup

setup: ## Initial setup - copy .env file
	@if [ ! -f .env ]; then \
		cp .env.example .env; \
		echo "‚úÖ Created .env file from .env.example"; \
		echo "‚ö†Ô∏è  Please review and adjust values in .env if needed"; \
	else \
		echo "‚ö†Ô∏è  .env file already exists, skipping"; \
	fi

validate: ## Validate environment and prerequisites
	@echo "üîç Validating environment..."
	@command -v docker >/dev/null 2>&1 || { echo "‚ùå Docker is not installed"; exit 1; }
	@command -v docker >/dev/null 2>&1 || { echo "‚ùå Docker Compose is not installed"; exit 1; }
	@[ -f .env ] || { echo "‚ùå .env file not found. Run 'make setup' first"; exit 1; }
	@echo "‚úÖ Environment validated"

##@ Docker Operations

up: validate ## Start all services
	@echo "üöÄ Starting Distributed Lakehouse..."
	docker compose up -d
	@echo ""
	@echo "‚úÖ Services started successfully!"
	@echo ""
	@echo "üìä Access points:"
	@echo "  - MinIO Console:    http://localhost:$(MINIO_CONSOLE_PORT) ($(MINIO_ROOT_USER)/$(MINIO_ROOT_PASSWORD))"
	@echo "  - Trino UI:         http://localhost:$(TRINO_COORDINATOR_PORT)"
	@echo "  - Airflow UI:       http://localhost:$(AIRFLOW_WEBSERVER_PORT) (admin/admin)"
	@echo ""
	@echo "‚è≥ Wait 30-60 seconds for all services to initialize"
	@echo "üìù Check logs: make logs"
	@echo "üîß Scale workers: make scale-workers WORKERS=3"

up-build: validate ## Start all services with rebuild
	@echo "üî® Building and starting services..."
	docker compose up -d --build

down: ## Stop all services
	@echo "üõë Stopping all services..."
	docker compose down
	@echo "‚úÖ Services stopped"

restart: ## Restart all services
	@echo "üîÑ Restarting services..."
	docker compose restart
	@echo "‚úÖ Services restarted"

scale-workers: ## Scale Trino workers (usage: make scale-workers WORKERS=3)
	@echo "‚öôÔ∏è  Scaling Trino workers to $(WORKERS)..."
	docker compose up -d --scale trino-worker=$(WORKERS)
	@echo "‚úÖ Trino workers scaled to $(WORKERS)"

##@ Logs

logs: ## Show logs from all services
	docker compose logs -f

logs-airflow: ## Show Airflow logs
	docker compose logs -f airflow-webserver airflow-scheduler

logs-trino: ## Show Trino logs
	docker compose logs -f trino-coordinator trino-worker

logs-metastore: ## Show Hive Metastore logs
	docker compose logs -f hive-metastore

logs-minio: ## Show MinIO logs
	docker compose logs -f minio

##@ Airflow

airflow-trigger: ## Trigger the NYC Taxi pipeline DAG
	@echo "‚ñ∂Ô∏è  Triggering NYC Taxi pipeline..."
	docker compose exec airflow-scheduler airflow dags trigger nyc_taxi_pipeline
	@echo "‚úÖ Pipeline triggered. Monitor at http://localhost:$(AIRFLOW_WEBSERVER_PORT)"

airflow-list-dags: ## List all Airflow DAGs
	docker compose exec airflow-scheduler airflow dags list

airflow-shell: ## Open Airflow scheduler shell
	docker compose exec airflow-scheduler /bin/bash

##@ dbt

dbt-debug: ## Run dbt debug
	docker compose exec airflow-scheduler dbt debug --project-dir $(DBT_PROJECT_DIR) --profiles-dir $(DBT_PROFILES_DIR)

dbt-run: ## Run dbt models
	docker compose exec airflow-scheduler dbt run --project-dir $(DBT_PROJECT_DIR) --profiles-dir $(DBT_PROFILES_DIR)

dbt-test: ## Run dbt tests
	docker compose exec airflow-scheduler dbt test --project-dir $(DBT_PROJECT_DIR) --profiles-dir $(DBT_PROFILES_DIR)

dbt-docs: ## Generate and serve dbt documentation
	docker compose exec airflow-scheduler dbt docs generate --project-dir $(DBT_PROJECT_DIR) --profiles-dir $(DBT_PROFILES_DIR)

##@ Trino

trino-cli: ## Open Trino CLI
	docker compose exec trino-coordinator trino --catalog iceberg --schema marts

trino-ui: ## Open Trino UI in browser
	@echo "Opening Trino UI..."
	@python3 -m webbrowser "http://localhost:$(TRINO_COORDINATOR_PORT)" 2>/dev/null || \
		echo "üìä Open manually: http://localhost:$(TRINO_COORDINATOR_PORT)"

##@ Demo

demo: ## Run demo queries (requires pipeline to be completed)
	@echo "üéØ Running demo queries..."
	@echo "üìä Check Trino UI to see distributed execution: http://localhost:$(TRINO_COORDINATOR_PORT)"
	docker compose exec -T trino-coordinator trino --catalog iceberg --schema marts -f /opt/airflow/scripts/demo_queries.sql

demo-1-worker: ## Demo with 1 worker
	@echo "üîß Running demo with 1 worker..."
	@$(MAKE) scale-workers WORKERS=1
	@sleep 10
	@echo "‚è±Ô∏è  Running queries..."
	@time $(MAKE) demo

demo-3-workers: ## Demo with 3 workers
	@echo "üîß Running demo with 3 workers..."
	@$(MAKE) scale-workers WORKERS=3
	@sleep 10
	@echo "‚è±Ô∏è  Running queries..."
	@time $(MAKE) demo

demo-compare: ## Compare performance: 1 worker vs 3 workers
	@echo "üìä Performance Comparison: 1 Worker vs 3 Workers"
	@echo "================================================"
	@echo ""
	@echo "‚ñ∂Ô∏è  Test 1: Single Worker"
	@$(MAKE) demo-1-worker
	@echo ""
	@echo "‚è∏Ô∏è  Waiting 15 seconds before scaling..."
	@sleep 15
	@echo ""
	@echo "‚ñ∂Ô∏è  Test 2: Three Workers"
	@$(MAKE) demo-3-workers
	@echo ""
	@echo "‚úÖ Comparison complete! Check execution times above."

##@ Monitoring

status: ## Show status of all services
	@echo "üìä Service Status:"
	@docker compose ps

health: ## Check health of all services
	@echo "üè• Health Check:"
	@docker compose ps --format "table {{.Name}}\t{{.Status}}"

minio-ui: ## Open MinIO Console in browser
	@echo "Opening MinIO Console..."
	@python3 -m webbrowser "http://localhost:$(MINIO_CONSOLE_PORT)" 2>/dev/null || \
		echo "üì¶ Open manually: http://localhost:$(MINIO_CONSOLE_PORT)"

airflow-ui: ## Open Airflow UI in browser
	@echo "Opening Airflow UI..."
	@python3 -m webbrowser "http://localhost:$(AIRFLOW_WEBSERVER_PORT)" 2>/dev/null || \
		echo "üå¨Ô∏è  Open manually: http://localhost:$(AIRFLOW_WEBSERVER_PORT)"

##@ Cleanup

clean: ## Remove containers and networks (keeps volumes)
	@echo "üßπ Cleaning up containers and networks..."
	docker compose down
	@echo "‚úÖ Cleanup complete (volumes preserved)"

clean-all: ## Remove everything including volumes (WARNING: deletes all data!)
	@echo "‚ö†Ô∏è  WARNING: This will delete ALL data including:"
	@echo "  - MinIO data (raw data and lakehouse)"
	@echo "  - Metastore database"
	@echo "  - Airflow database"
	@echo ""
	@read -p "Are you sure? Type 'yes' to continue: " confirm; \
	if [ "$$confirm" = "yes" ]; then \
		echo "üßπ Removing everything..."; \
		docker compose down -v; \
		rm -rf airflow/logs/*; \
		echo "‚úÖ Complete cleanup done"; \
	else \
		echo "‚ùå Cleanup cancelled"; \
	fi

clean-logs: ## Clean Airflow logs
	@echo "üßπ Cleaning Airflow logs..."
	rm -rf airflow/logs/*
	@echo "‚úÖ Logs cleaned"

##@ Troubleshooting

debug-metastore: ## Debug Hive Metastore connection
	@echo "üîç Testing Metastore connection..."
	docker compose exec postgres-metastore psql -U $(POSTGRES_METASTORE_USER) -d $(POSTGRES_METASTORE_DB) -c "\dt"

debug-minio: ## Check MinIO buckets
	@echo "üîç Checking MinIO buckets..."
	docker compose exec minio-mc mc ls myminio

debug-trino-catalogs: ## Show Trino catalogs
	@echo "üîç Showing Trino catalogs..."
	docker compose exec trino-coordinator trino --execute "SHOW CATALOGS;"

debug-trino-schemas: ## Show Trino schemas in all catalogs
	@echo "üîç Showing Trino schemas..."
	@echo "--- Hive Catalog ---"
	docker compose exec trino-coordinator trino --catalog hive --execute "SHOW SCHEMAS;"
	@echo ""
	@echo "--- Iceberg Catalog ---"
	docker compose exec trino-coordinator trino --catalog iceberg --execute "SHOW SCHEMAS;"

troubleshoot: ## Run all troubleshooting checks
	@echo "üîß Running comprehensive troubleshooting..."
	@echo ""
	@$(MAKE) status
	@echo ""
	@$(MAKE) debug-minio
	@echo ""
	@$(MAKE) debug-metastore
	@echo ""
	@$(MAKE) debug-trino-catalogs
