# ğŸ—ï¸ Distributed Lakehouse Template

> **Template pronto para uso** de data lakehouse moderna com Docker: Trino + Iceberg + MinIO + dbt + Airflow

Clone este repositÃ³rio e customize para criar sua prÃ³pria pipeline de dados em minutos! Inclui exemplo funcional com dados NYC Taxi.

**Pipeline completo**: ingestÃ£o â†’ transformaÃ§Ã£o â†’ consumo com arquitetura medallion (Raw â†’ Staging â†’ Marts)

---

## ğŸ¯ Como Usar

### **Clone e customize para sua pipeline:**

```bash
# 1. Clone o template
git clone <repo> my-lakehouse
cd my-lakehouse

# 2. Customize seus scripts
cp templates/ingest_template.py scripts/ingest_my_data.py
vim scripts/ingest_my_data.py  # Implemente extract_data()

# 3. Suba o ambiente
docker compose up -d

# 4. Teste sua ingestÃ£o
docker exec airflow-webserver python3 /opt/airflow/scripts/ingest_my_data.py
```

ğŸ“˜ **[Guia Completo de CustomizaÃ§Ã£o â†’](SETUP_YOUR_PIPELINE.md)**

ğŸ’¡ **Exemplos de referÃªncia** disponÃ­veis em `examples/` (dados NYC Taxi)

---

## ğŸ“Š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Camada de Consumo (SQL Clients)                        â”‚
â”‚  DBeaver, Python, Jupyter                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (SQL Queries)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trino (Distributed Query Engine)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Hive Catalog  â”‚        â”‚  Iceberg Catalog â”‚        â”‚
â”‚  â”‚  (Raw Layer)   â”‚        â”‚  (Marts Layer)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hive Metastore (PostgreSQL)                          â”‚
â”‚  CatÃ¡logo unificado de metadados                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MinIO (S3-compatible Object Storage)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  raw/        â”‚    â†’    â”‚  warehouse/         â”‚    â”‚
â”‚  â”‚  153K rows   â”‚  (dbt)  â”‚  149K rows          â”‚    â”‚
â”‚  â”‚  Parquet     â”‚         â”‚  Iceberg Tables     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow (OrquestraÃ§Ã£o)                               â”‚
â”‚  ingest â†’ create tables â†’ dbt run â†’ dbt test          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados (Medallion Architecture)

```
Raw (Hive)          Staging (dbt)       Marts (Iceberg)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
hive.raw            iceberg.staging     iceberg.marts
â”œâ”€ my_table         â””â”€ stg_*            â”œâ”€ fct_*
â”‚  Parquet simples     (views)          â”‚  (tables)
â”‚  ImutÃ¡vel                             â”œâ”€ dim_*
â”‚                                          ACID + Time Travel
```

---

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Docker + Docker Compose
- 8GB RAM mÃ­nimo
- Portas disponÃ­veis: 8080, 8081, 9000, 9001, 5432, 5433, 9083

### 1. Clone e configure

```bash
git clone <seu-repo> my-lakehouse
cd my-lakehouse

# Copie e ajuste variÃ¡veis
cp .env.example .env
```

### 2. Customize sua pipeline

```bash
# Veja o guia completo
cat SETUP_YOUR_PIPELINE.md

# Ou use os templates
cp templates/ingest_template.py scripts/ingest_my_data.py
```

### 3. Suba o ambiente

```bash
docker compose up -d

# Aguarde ~30 segundos
docker compose ps
```

### 4. Acesse os dados

#### Via Trino CLI

```bash
# Entrar no CLI interativo
docker exec -it trino-coordinator trino --catalog iceberg --schema marts

# Executar queries
trino:marts> SELECT COUNT(*) FROM fct_trips;
# 149848

trino:marts> SELECT
    pickup_date_key,
    COUNT(*) as trips,
    ROUND(AVG(fare_amount), 2) as avg_fare
FROM fct_trips
GROUP BY pickup_date_key
ORDER BY pickup_date_key DESC
LIMIT 5;
```

#### Via DBeaver/DataGrip

1. **Baixar o driver JDBC:**
   - Driver estÃ¡ em: `drivers/trino-jdbc-438.jar`
   - Ou download: https://repo1.maven.org/maven2/io/trino/trino-jdbc/438/trino-jdbc-438.jar

2. **Criar conexÃ£o:**
   ```
   Host:      localhost
   Port:      8080
   Database:  iceberg
   Schema:    marts
   Username:  admin
   Password:  (vazio)
   Driver:    Trino JDBC
   ```

3. **Executar queries:**
   ```sql
   SELECT * FROM iceberg.marts.fct_trips LIMIT 100;
   ```

Guia completo: [`docs/CONNECT_DBEAVER.md`](docs/CONNECT_DBEAVER.md)

#### Via Python

```python
import trino
import pandas as pd

conn = trino.dbapi.connect(
    host='localhost',
    port=8080,
    user='admin',
    catalog='iceberg',
    schema='marts'
)

df = pd.read_sql("SELECT * FROM fct_trips LIMIT 100", conn)
print(df.head())
```

---

## ğŸ¯ Interfaces Web

| ServiÃ§o | URL | DescriÃ§Ã£o |
|---------|-----|-----------|
| **Airflow** | http://localhost:8081 | OrquestraÃ§Ã£o e monitoramento de pipelines |
| **Trino UI** | http://localhost:8080 | Monitor de queries e cluster status |
| **MinIO Console** | http://localhost:9001 | Browser de arquivos S3 (user: minioadmin / pass: minioadmin) |
| **dbt Docs** | http://localhost:8083 | DocumentaÃ§Ã£o interativa dos modelos dbt |

---

## ğŸ“ Estrutura do Projeto

```
distributed-lakehouse/
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ nyc_taxi_pipeline.py    # DAG principal
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â””â”€â”€ stg_nyc_taxi_trips.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â””â”€â”€ int_trips_enriched.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ fct_trips.sql
â”‚   â”‚       â”œâ”€â”€ dim_dates.sql
â”‚   â”‚       â””â”€â”€ dim_locations.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_nyc_taxi.py      # IngestÃ£o de dados
â”‚   â”œâ”€â”€ bootstrap_tables.py     # CriaÃ§Ã£o de schemas/tabelas
â”‚   â”œâ”€â”€ run_dbt.py              # Executor do dbt
â”‚   â””â”€â”€ query_example.py        # Exemplo de queries Python
â”‚
â”œâ”€â”€ trino/
â”‚   â”œâ”€â”€ coordinator/
â”‚   â”‚   â””â”€â”€ config.properties
â”‚   â”œâ”€â”€ worker/
â”‚   â”‚   â””â”€â”€ config.properties
â”‚   â””â”€â”€ catalog/
â”‚       â”œâ”€â”€ hive.properties     # CatÃ¡logo para raw
â”‚       â””â”€â”€ iceberg.properties  # CatÃ¡logo para marts
â”‚
â”œâ”€â”€ metastore/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ entrypoint.sh
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARQUITETURA_COMPLETA.md  # Detalhes tÃ©cnicos completos
    â”œâ”€â”€ CONNECT_DBEAVER.md       # Guia de conexÃ£o SQL clients
    â””â”€â”€ HIVE_VS_ICEBERG.sql      # DiferenÃ§as entre catÃ¡logos
```

---

## ğŸ”§ Comandos Ãšteis

### Gerenciar containers

```bash
# Ver status
docker compose ps

# Parar tudo
docker compose down

# Limpar volumes (apaga dados)
docker compose down -v

# Ver logs
docker compose logs -f trino-coordinator
docker compose logs -f airflow-scheduler
```

### Acessar containers

```bash
# Trino CLI
docker exec -it trino-coordinator trino

# Airflow bash
docker exec -it airflow-webserver bash

# MinIO CLI
docker exec minio mc ls minio/lakehouse/ --recursive
```

### Pipeline Airflow

```bash
# Listar DAGs
docker exec airflow-webserver airflow dags list

# Trigger manual
docker exec airflow-webserver airflow dags trigger nyc_taxi_pipeline

# Ver runs
docker exec airflow-webserver airflow dags list-runs --dag-id nyc_taxi_pipeline

# Ver status das tasks
docker exec airflow-webserver airflow tasks states-for-dag-run \
  nyc_taxi_pipeline <run_id>
```

### dbt

```bash
# Rodar modelos
docker exec airflow-webserver bash -c "cd /opt/airflow/dbt && dbt run --profiles-dir ."

# Rodar testes
docker exec airflow-webserver bash -c "cd /opt/airflow/dbt && dbt test --profiles-dir ."

# Gerar documentaÃ§Ã£o
docker exec airflow-webserver bash -c "cd /opt/airflow/dbt && dbt docs generate --profiles-dir ."
```

---

## ğŸ’¡ Conceitos Importantes

### Por que 2 catÃ¡logos (Hive e Iceberg)?

Ambos usam o **mesmo Hive Metastore** (PostgreSQL) como backend de metadados, mas:

- **CatÃ¡logo Hive**: Para tabelas raw (Parquet simples, imutÃ¡veis)
  - `hive.raw.nyc_taxi_trips`
  - Sem ACID, sem time travel
  - Formato mais simples e compatÃ­vel

- **CatÃ¡logo Iceberg**: Para tabelas marts (Iceberg tables)
  - `iceberg.marts.fct_trips`
  - ACID transactions
  - Time travel (ver versÃµes antigas)
  - Schema evolution

**Regra de ouro:**
```sql
-- âœ… Correto
SELECT * FROM hive.raw.nyc_taxi_trips;       -- Raw data
SELECT * FROM iceberg.marts.fct_trips;       -- Marts

-- âŒ Evitar
SELECT * FROM iceberg.raw.nyc_taxi_trips;    -- DÃ¡ erro
SELECT * FROM hive.marts.fct_trips;          -- Funciona mas perde features Iceberg
```

Mais detalhes: [`docs/HIVE_VS_ICEBERG.sql`](docs/HIVE_VS_ICEBERG.sql)

### Camadas de dados (Medallion)

```
Raw      â†’ Dados originais imutÃ¡veis (Hive external table)
Staging  â†’ Limpeza e padronizaÃ§Ã£o (dbt views)
Marts    â†’ Modelos finais otimizados para BI (Iceberg tables)
```

---

## ğŸ’¡ Exemplos de ReferÃªncia

O diretÃ³rio `examples/` contÃ©m uma pipeline completa funcional com dados NYC Taxi:

```
examples/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_nyc_taxi.py              # IngestÃ£o de API
â”‚   â””â”€â”€ bootstrap_tables_nyc_taxi.py    # Schema completo
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ nyc_taxi_pipeline.py            # DAG funcional
â””â”€â”€ dbt_models/
    â”œâ”€â”€ stg_nyc_taxi_trips.sql          # Staging
    â”œâ”€â”€ int_trips_enriched.sql          # Intermediate
    â”œâ”€â”€ fct_trips.sql                   # Facts
    â”œâ”€â”€ dim_dates.sql                   # Dimensions
    â””â”€â”€ sources.yml                     # Sources config
```

**Use como referÃªncia** ao implementar sua prÃ³pria pipeline!

---

## ğŸ› Troubleshooting

### Containers nÃ£o sobem

```bash
# Verificar portas em uso
netstat -tulpn | grep -E "8080|8081|9000|5432"

# Ver logs
docker compose logs
```

### Airflow DAG nÃ£o aparece

```bash
# Verificar permissÃµes
ls -la airflow/dags/

# Restart do scheduler
docker compose restart airflow-scheduler
```

### Trino connection refused

```bash
# Verificar se estÃ¡ healthy
docker compose ps trino-coordinator

# Aguardar 30s apÃ³s startup
sleep 30 && docker exec trino-coordinator trino --execute "SELECT 1"
```

### dbt tests falhando

```bash
# Ver logs detalhados
docker exec airflow-webserver bash -c "cd /opt/airflow/dbt && dbt test --profiles-dir . --debug"
```

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[Arquitetura Completa](docs/ARQUITETURA_COMPLETA.md)**: Detalhes tÃ©cnicos, diagramas, estatÃ­sticas
- **[Conectar DBeaver](docs/CONNECT_DBEAVER.md)**: Guia passo a passo para SQL clients
- **[Hive vs Iceberg](docs/HIVE_VS_ICEBERG.sql)**: DiferenÃ§as entre catÃ¡logos com exemplos

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Veja [`CONTRIBUTING.md`](CONTRIBUTING.md) para guidelines.

---

## ğŸ“ LicenÃ§a

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

## ğŸ™‹ Suporte

Problemas ou dÃºvidas? Abra uma [issue](https://github.com/yourusername/distributed-lakehouse/issues).

---

**Feito com â¤ï¸ usando Trino, Iceberg, MinIO, dbt e Airflow**
